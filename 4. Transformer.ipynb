{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9134419",
   "metadata": {},
   "source": [
    "## Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0928c",
   "metadata": {},
   "source": [
    "### Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afe1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('DMC-Train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b97871",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('DMC-phase2-validation.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc3d5b",
   "metadata": {},
   "source": [
    "### Unwrap JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b29f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4731b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_json(dataframe):\n",
    "    post_df = pd.json_normalize(dataframe.post_data.apply(json.loads))\n",
    "    post_df = post_df.drop(post_df.columns[15:], axis=1)\n",
    "    dataframe = pd.concat([post_df, dataframe], axis=1)\n",
    "    dataframe = dataframe.drop('post_data', axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2641b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = unwrap_json(train_df)\n",
    "\n",
    "train_df['reject_reason_id'] = train_df['reject_reason_id'].map({\n",
    "    0: 0, 5: 1, 12: 2, 13: 3, 29: 4, 139: 5, 145: 6, 146: 7, 163: 8\n",
    "})\n",
    "\n",
    "train_label = train_df[['reject_reason_id']].to_numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9deca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = unwrap_json(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b8b96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540362,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f962251",
   "metadata": {},
   "source": [
    "### Categorical to One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c427541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03811ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_onehot(dataframe):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(dataframe)\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45925cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['body_status', 'brand_model', 'color', 'document',\n",
    "               'gearbox', 'selling_type', 'year', 'third_party_insurance_deadline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c3bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_model = train_onehot(train_df[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = onehot_model.transform(train_df[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b747e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = onehot_model.transform(test_df[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4320cfc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540362, 1252)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadad7e",
   "metadata": {},
   "source": [
    "### Numerical to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc949bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_num2cat(dataframe, num_bins):\n",
    "    bins = pd.qcut(dataframe, duplicates='drop', q=num_bins, retbins=True)[1]\n",
    "    bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "    res_df = pd.cut(dataframe, bins).to_frame()\n",
    "    oh_model = train_onehot(res_df)\n",
    "    return bins, oh_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6317006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_num2cat(dataframe, model, bins):\n",
    "    res_df = pd.cut(dataframe, bins).to_frame()\n",
    "    return model.transform(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2af344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_bins, price_model = train_num2cat(train_df['new_price'], 50)\n",
    "usage_bins, usage_model = train_num2cat(train_df['usage'], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b6f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_price = transform_num2cat(train_df['new_price'], price_model, price_bins)\n",
    "train_usage = transform_num2cat(train_df['usage'], usage_model, usage_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ba5ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540362, 51)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8919f41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540362, 23)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_usage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c2a19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = hstack((train_cat, train_price, train_usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c48b2473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540362, 1326)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c93fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_price = transform_num2cat(test_df['new_price'], price_model, price_bins)\n",
    "test_usage = transform_num2cat(test_df['usage'], usage_model, usage_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "136971c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = hstack((test_cat, test_price, test_usage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15208a",
   "metadata": {},
   "source": [
    "### Cleaning Text Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "663dd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "from hazm import Normalizer, WordTokenizer, Lemmatizer\n",
    "import re\n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a320c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_translation_dict.pickle', 'rb') as handle:\n",
    "    new_translation = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cad6a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.dat') as handle:\n",
    "    stopwords = handle.readlines()\n",
    "    stopwords = [word[:-1] for word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "490ba89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHandler:\n",
    "    def __init__(self, persian_numbers=False,\n",
    "                 change_lang_spacing=True,\n",
    "                 remove_non_standard_char=True,\n",
    "                 remove_repetitive_chars=True,\n",
    "                 user_translations=None,\n",
    "                 stopwords=None,\n",
    "                 lemma=False\n",
    "                ):\n",
    "        \n",
    "        # text preprocessing config\n",
    "        if not persian_numbers:\n",
    "            number_src = '۰۱۲۳۴۵۶۷۸۹٪'\n",
    "            number_dest = '0123456789%'\n",
    "        else:\n",
    "            number_dest = '۰۱۲۳۴۵۶۷۸۹٪'\n",
    "            number_src = '0123456789%'\n",
    "        \n",
    "        self.number_translations = self.maketrans(number_src, number_dest)\n",
    "        \n",
    "        if not user_translations:\n",
    "            self.user_translations = dict()\n",
    "        else:\n",
    "            self.user_translations = user_translations\n",
    "\n",
    "        self._remove_repetitive_chars = remove_repetitive_chars\n",
    "        self._change_lang_spacing = change_lang_spacing\n",
    "        self._remove_non_standard_char = remove_non_standard_char\n",
    "        self._stopwords = stopwords\n",
    "        \n",
    "        self.text_normalizer = hazm.Normalizer(\n",
    "            remove_extra_spaces=True,\n",
    "            persian_style=False,\n",
    "            persian_numbers=False,\n",
    "            remove_diacritics=True,\n",
    "            affix_spacing=True,\n",
    "            token_based=False,\n",
    "            punctuation_spacing=True)\n",
    "\n",
    "        self.word_tokenizer = hazm.WordTokenizer(\n",
    "            join_verb_parts=False,\n",
    "            separate_emoji=True,\n",
    "            replace_links=True,\n",
    "            replace_IDs=False,\n",
    "            replace_emails=True,\n",
    "            replace_numbers=False,\n",
    "            replace_hashtags=False)\n",
    "        \n",
    "        if lemma:\n",
    "            self.lemmatizer = Lemmatizer()\n",
    "        else:\n",
    "            self.lemmatizer = None\n",
    "        \n",
    "\n",
    "    def normalize(self, text: str):\n",
    "        text = text.translate(self.user_translations)\n",
    "        text = text.translate(self.number_translations)\n",
    "        \n",
    "        text = text.lower()\n",
    "\n",
    "        normalized_text = self.text_normalizer.normalize(text)\n",
    "\n",
    "        if self._remove_repetitive_chars:\n",
    "            text = self.remove_rep_chars(text)\n",
    "\n",
    "        if self._change_lang_spacing:\n",
    "            text = self.change_lang_spacing(text)\n",
    "\n",
    "        if self._remove_non_standard_char:\n",
    "            text = self.remove_non_standard_char(text)\n",
    "            \n",
    "        text = self.remove_stopwords_and_lemma(text)\n",
    "\n",
    "        # reduce multiple spaces to one space\n",
    "        text = re.sub(r'[\\u200c\\s]*\\s[\\s\\u200c]*', ' ', text)\n",
    "        text = re.sub(r'[\\u200c]+', '\\u200c', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def maketrans(src_chars, dest_chars):\n",
    "        return dict((ord(a), b) for a, b in zip(src_chars, dest_chars))\n",
    "    \n",
    "    @staticmethod\n",
    "    def change_lang_spacing(text: str) -> str:\n",
    "        return re.sub('(([a-zA-Z0-9/\\-\\.]+)|([ء-یژپچگ]+))', r' \\1 ', text).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_non_standard_char(text: str) -> str:\n",
    "        # replace every junk character with space (all characters except Persian and English chars plus English digits)\n",
    "        return re.sub(r'[^a-zA-Z0-9\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF]', ' ', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_rep_chars(text: str) -> str:\n",
    "        return re.sub(r'([^0-9])\\1\\1+', r'\\1', text)\n",
    "    \n",
    "    def remove_stopwords_and_lemma(self, text: str) -> str:\n",
    "        if self.lemmatizer or self._stopwords:\n",
    "            words = self.word_tokenizer.tokenize(text)\n",
    "            if self._stopwords:\n",
    "                words = [w for w in words if w not in self._stopwords]\n",
    "            if self.lemmatizer:\n",
    "                words = [self.lemmatizer.lemmatize(w) for w in words]\n",
    "            return ' '.join(words)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text: str):\n",
    "        normalized_text = self.normalize(text)\n",
    "        return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c8f0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_handler = TextHandler(user_translations=new_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bca34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['prep_title'] = train_df['title'].apply(text_handler.preprocess_text)\n",
    "train_df['prep_description'] = train_df['description'].apply(text_handler.preprocess_text)\n",
    "train_df['prep_text'] = '<start> ' + train_df['prep_description'] + ' <delim> ' + train_df['prep_title'] + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3715be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prep_title'] = test_df['title'].apply(text_handler.preprocess_text)\n",
    "test_df['prep_description'] = test_df['description'].apply(text_handler.preprocess_text)\n",
    "test_df['prep_text'] = '<start> ' + test_df['prep_description'] + ' <delim> ' + test_df['prep_title'] + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4aeae4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['prep_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ebe1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df['prep_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e1cec",
   "metadata": {},
   "source": [
    "### Text Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be901dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "tokenizer = hazm.WordTokenizer(\n",
    "            join_verb_parts=False,\n",
    "            separate_emoji=True,\n",
    "            replace_links=True,\n",
    "            replace_IDs=False,\n",
    "            replace_emails=True,\n",
    "            replace_numbers=False,\n",
    "            replace_hashtags=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9faed484",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = [tokenizer.tokenize(t) for t in train_texts]\n",
    "lens = [len(t) for t in train_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af6578da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lens, cumulative=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00c50d",
   "metadata": {},
   "source": [
    "### Saving to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfca4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b142d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(train_texts, f)\n",
    "    \n",
    "save_npz(\"train_cat.npz\", train_cat)\n",
    "\n",
    "with open('train_label.npy', 'wb') as f:\n",
    "    np.save(f, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a847f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(test_texts, f)\n",
    "    \n",
    "save_npz(\"test_cat.npz\", test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357092d7",
   "metadata": {},
   "source": [
    "### Loading from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226614b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7cbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_texts.pkl', 'rb') as f:\n",
    "    train_texts = pickle.load(f)\n",
    "    \n",
    "train_cat = load_npz(\"train_cat.npz\")\n",
    "\n",
    "with open('train_label.npy', 'rb') as f:\n",
    "    train_label = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a21cd",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34406050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91605aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train_cat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94bce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60957c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train_texts)\n",
    "indices = np.arange(train_len)\n",
    "np.random.shuffle(indices, )\n",
    "train_idx = indices[:math.floor(0.9*train_len)]\n",
    "val_idx = indices[math.floor(0.9*train_len):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f15012",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = [train_texts[i] for i in val_idx]\n",
    "val_cat = train_cat[val_idx]\n",
    "val_label = train_label[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca77c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [train_texts[i] for i in train_idx]\n",
    "train_cat = train_cat[train_idx]\n",
    "train_label = train_label[train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebcd78",
   "metadata": {},
   "source": [
    "### Creating PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f6d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81c1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_df):\n",
    "        self.itos = {0: '<pad>', 1: '<unk>'}\n",
    "        self.stoi = {'<pad>': 0, '<unk>': 1}\n",
    "        self.min_df = min_df\n",
    "        self.tokenizer = hazm.WordTokenizer(\n",
    "            join_verb_parts=False,\n",
    "            separate_emoji=True,\n",
    "            replace_links=True,\n",
    "            replace_IDs=False,\n",
    "            replace_emails=True,\n",
    "            replace_numbers=False,\n",
    "            replace_hashtags=False\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 2\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer.tokenize(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.min_df:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        \n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        numer_text = [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi['<unk>']\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "        \n",
    "        numer_text = numer_text[:118]\n",
    "        numer_text += (118 - len(numer_text)) * [0]\n",
    "        \n",
    "        return numer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae72db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivarDataset(Dataset):\n",
    "    def __init__(self, cat_mat, text_list, labels, vocab):\n",
    "        self.cat_mat = cat_mat\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "            \n",
    "        self.text_list = [self.vocab.numericalize(text) for text in text_list]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        cat_row = self.cat_mat[index]\n",
    "        numer_text = self.text_list[index]\n",
    "        label = self.labels[index]\n",
    "        return T.tensor(cat_row, dtype=T.float32), T.tensor(numer_text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a562ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        cat_rows = [item[0] for item in batch]\n",
    "        cat_rows = T.vstack(cat_rows)\n",
    "        numer_texts = [item[1] for item in batch]\n",
    "        text_lengths = [text.shape[0] for text in numer_texts]\n",
    "        numer_texts = pad_sequence(numer_texts, batch_first=True, padding_value=self.pad_idx)\n",
    "        labels = [item[2] for item in batch]\n",
    "        labels = T.tensor(labels, dtype=T.long)\n",
    "        \n",
    "        return cat_rows, numer_texts, labels, text_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012df394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(cat_mat, text_list, labels, vocab, batch_size=32, shuffle=True):\n",
    "    \n",
    "    dataset = DivarDataset(cat_mat, text_list, labels, vocab)\n",
    "    \n",
    "    pad_idx = dataset.vocab.stoi['<pad>']\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=Collate(pad_idx=pad_idx)\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f287444",
   "metadata": {},
   "source": [
    "### Creating Loader Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e28f3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5f8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=30\n",
    "vocab = Vocabulary(min_df)\n",
    "vocab.build_vocabulary(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f36d1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(train_cat, train_texts, train_label, vocab, batch_size=950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75673737",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = get_loader(val_cat, val_texts, val_label, vocab, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0672993",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e21ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Compact-Transformers'...\n",
      "remote: Enumerating objects: 418, done.\u001b[K\n",
      "remote: Counting objects: 100% (418/418), done.\u001b[K\n",
      "remote: Compressing objects: 100% (297/297), done.\u001b[K\n",
      "remote: Total 418 (delta 218), reused 251 (delta 112), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (418/418), 1.33 MiB | 222.00 KiB/s, done.\n",
      "Resolving deltas: 100% (218/218), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SHI-Labs/Compact-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee8fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv Compact-Transformers ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d4c0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from ct.src.text import text_cct_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f5ba347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_mask(seq_len, max_len):\n",
    "    idx = T.arange(max_len).to(seq_len).repeat(seq_len.size(0), 1)\n",
    "    mask = T.gt(seq_len.unsqueeze(1), idx).to(seq_len)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9224894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, cat_dim, dict_dim):\n",
    "        super().__init__()              \n",
    "            \n",
    "        self.transformer = text_cct_2(kernel_size=1, seq_len=116, padding_idx=0,\n",
    "                                      vocab_size=dict_dim, num_classes=300, word_embedding_dim=300)\n",
    "        \n",
    "        self.cat_shrink = nn.Sequential(\n",
    "            nn.Linear(cat_dim, 300),\n",
    "            nn.Tanh(),            \n",
    "        )\n",
    "                \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 9)\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, cat, text, mask):\n",
    "        \n",
    "        text_embed = T.tanh(self.transformer(text, mask))\n",
    "\n",
    "        cat_shrinked = self.cat_shrink(cat)\n",
    "        \n",
    "        lin_input = T.cat((cat_shrinked, text_embed), dim=1)\n",
    "        \n",
    "        return self.fc(lin_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc636b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_DIM = train_cat.shape[1]\n",
    "DICT_DIM = vocab.__len__()\n",
    "model = Classifier(CAT_DIM, DICT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969d4cb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e5bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebd0a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(weight=T.tensor([1.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b76b0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "criterion = criterion.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37243e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = T.round(preds)\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4790142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    for i, (cat, text, label, text_lengths) in enumerate(iterator):\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f'    mini-batch {i}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cat = cat.to(device)\n",
    "        text = text.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mask = seq_mask(T.tensor(text_lengths), 118).to(device)\n",
    "        \n",
    "        predictions = model(cat, text, mask)\n",
    "        loss = criterion(predictions, label)\n",
    "        \n",
    "        softmax_predictions = F.softmax(predictions, dim=1)\n",
    "        binary_predictions = 1 - softmax_predictions[:, 0]\n",
    "        binary_label = (label != 0).float()\n",
    "        acc = binary_accuracy(binary_predictions, binary_label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(text)\n",
    "        epoch_acc += acc.item() * len(text)\n",
    "                \n",
    "    return epoch_loss / len(iterator.dataset), epoch_acc / len(iterator.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abfe4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    with T.no_grad():\n",
    "    \n",
    "        for i, (cat, text, label, text_lengths) in enumerate(iterator):\n",
    "\n",
    "            cat = cat.to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            mask = seq_mask(T.tensor(text_lengths), 118).to(device)\n",
    "        \n",
    "            predictions = model(cat, text, mask).squeeze(1)\n",
    "                            \n",
    "            loss = criterion(predictions, label)\n",
    "            \n",
    "            softmax_predictions = F.softmax(predictions, dim=1)\n",
    "            binary_predictions = 1 - softmax_predictions[:, 0]\n",
    "            binary_label = (label != 0).float()\n",
    "            acc = binary_accuracy(binary_predictions, binary_label)\n",
    "            \n",
    "            preds += binary_predictions.tolist()\n",
    "            labels += binary_label.tolist()\n",
    "\n",
    "            epoch_loss += loss.item() * len(text)\n",
    "            epoch_acc += acc.item() * len(text)\n",
    "            \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "        \n",
    "    return epoch_loss / len(iterator.dataset), epoch_acc / len(iterator.dataset), auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87332674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bf4969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch, warmup=4, epochs=12, lr=0.001):\n",
    "    if epoch < warmup:\n",
    "        lr = lr / (warmup - epoch)\n",
    "    else:\n",
    "        lr *= 0.5 * (1. + math.cos(math.pi * (epoch - warmup) / (epochs - warmup)))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63496bbe",
   "metadata": {},
   "source": [
    "CCT2, emb=300, word=300, layer=2, conv_pool=true, class_token, lr=0.0005, batch=450, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b360586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.922 | Train Acc: 82.11%\n",
      "     Val. Loss: 0.636 |  Val. Acc: 88.38% | Val. AUC: 0.9259\n",
      "\n",
      "Epoch: 2\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.597 | Train Acc: 88.30%\n",
      "     Val. Loss: 0.574 |  Val. Acc: 90.23% | Val. AUC: 0.9366\n",
      "\n",
      "Epoch: 3\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.543 | Train Acc: 89.16%\n",
      "     Val. Loss: 0.524 |  Val. Acc: 89.36% | Val. AUC: 0.9413\n",
      "\n",
      "Epoch: 4\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 42s\n",
      "    Train Loss: 0.522 | Train Acc: 89.81%\n",
      "     Val. Loss: 0.509 |  Val. Acc: 90.95% | Val. AUC: 0.9466\n",
      "\n",
      "Epoch: 5\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 43s\n",
      "    Train Loss: 0.474 | Train Acc: 90.94%\n",
      "     Val. Loss: 0.484 |  Val. Acc: 91.34% | Val. AUC: 0.9502\n",
      "\n",
      "Epoch: 6\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 42s\n",
      "    Train Loss: 0.436 | Train Acc: 91.78%\n",
      "     Val. Loss: 0.463 |  Val. Acc: 92.00% | Val. AUC: 0.9574\n",
      "\n",
      "Epoch: 7\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.404 | Train Acc: 92.28%\n",
      "     Val. Loss: 0.443 |  Val. Acc: 92.41% | Val. AUC: 0.9606\n",
      "\n",
      "Epoch: 8\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.372 | Train Acc: 92.78%\n",
      "     Val. Loss: 0.427 |  Val. Acc: 92.42% | Val. AUC: 0.9626\n",
      "\n",
      "Epoch: 9\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.339 | Train Acc: 93.28%\n",
      "     Val. Loss: 0.415 |  Val. Acc: 92.50% | Val. AUC: 0.9643\n",
      "\n",
      "Epoch: 10\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.308 | Train Acc: 93.81%\n",
      "     Val. Loss: 0.417 |  Val. Acc: 92.87% | Val. AUC: 0.9657\n",
      "\n",
      "Epoch: 11\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 40s\n",
      "    Train Loss: 0.281 | Train Acc: 94.25%\n",
      "     Val. Loss: 0.421 |  Val. Acc: 92.97% | Val. AUC: 0.9661\n",
      "\n",
      "Epoch: 12\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "Epoch Time: 7m 41s\n",
      "    Train Loss: 0.262 | Train Acc: 94.59%\n",
      "     Val. Loss: 0.420 |  Val. Acc: 92.84% | Val. AUC: 0.9662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 12\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    adjust_lr(optimizer, epoch)\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_auc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    T.save(model.state_dict(), f'360_{epoch}.torch')\n",
    "    T.save(optimizer.state_dict(), 'optim_360_{epoch}.torch')\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "       \n",
    "    print(f'Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'    Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'     Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. AUC: {valid_auc:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e19df",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91e6408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    with T.no_grad():\n",
    "    \n",
    "        for i, (cat, text, label, text_lengths) in enumerate(iterator):\n",
    "\n",
    "            cat = cat.to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            mask = seq_mask(T.tensor(text_lengths), 256).to(device)\n",
    "            \n",
    "            predictions = model(cat, text, mask).squeeze(1)        \n",
    "            predictions = T.max(predictions, dim=1)[1]\n",
    "            \n",
    "            preds += predictions.tolist()\n",
    "            labels += label.tolist()\n",
    "        \n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04b7f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adf773a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, labels = predict(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bfd517",
   "metadata": {},
   "source": [
    "0: 0, 5: 1, 12: 2, 13: 3, 29: 4, 139: 5, 145: 6, 146: 7, 163: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3eb716a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95     41471\n",
      "           1       0.79      0.84      0.81       170\n",
      "           2       0.83      0.89      0.86      2740\n",
      "           3       0.80      0.80      0.80      2143\n",
      "           4       0.69      0.59      0.64       137\n",
      "           5       0.53      0.61      0.57      2158\n",
      "           6       0.95      0.95      0.95      2702\n",
      "           7       0.75      0.71      0.73       267\n",
      "           8       0.70      0.70      0.70      2249\n",
      "\n",
      "    accuracy                           0.91     54037\n",
      "   macro avg       0.78      0.78      0.78     54037\n",
      "weighted avg       0.92      0.91      0.91     54037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7838e",
   "metadata": {},
   "source": [
    "### Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a8d9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.save(model.state_dict(), 'transformer.torch')\n",
    "T.save(optimizer.state_dict(), 'transformer_optim.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18e11343",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_stoi.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab.stoi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e8a8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_itos.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab.itos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cc58e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326 9654\n"
     ]
    }
   ],
   "source": [
    "print(CAT_DIM, DICT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74507902",
   "metadata": {},
   "source": [
    "### Loading Test Data & The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5084c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "import hazm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2647126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_texts.pkl', 'rb') as f:\n",
    "    test_texts = pickle.load(f)\n",
    "    \n",
    "test_cat = load_npz(\"test_cat.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afad0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = test_cat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7517ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_stoi.pkl', 'rb') as f:\n",
    "    vocab_stoi = pickle.load(f)\n",
    "\n",
    "with open('vocab_itos.pkl', 'rb') as f:\n",
    "    vocab_itos = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc5645fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(30)\n",
    "vocab.stoi = vocab_stoi\n",
    "vocab.itos = vocab_itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5bb23aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_loader(test_cat, test_texts, np.zeros(len(test_texts)), vocab, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad67c649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAT_DIM = 1371 \n",
    "DICT_DIM = 9589\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 400\n",
    "dropout = 0.3\n",
    "\n",
    "model = Classifier(CAT_DIM, DICT_DIM, EMBEDDING_DIM, HIDDEN_DIM, dropout)\n",
    "model.load_state_dict(T.load('transformer.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(T.load('transformer_optim.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58fc69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2396e",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7bfbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65ea3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    with T.no_grad():\n",
    "        for i, (cat, text, label, text_lengths) in enumerate(iterator):\n",
    "\n",
    "            cat = cat.to(device)\n",
    "            text = text.to(device)\n",
    "            \n",
    "            mask = seq_mask(T.tensor(text_lengths), 256).to(device)\n",
    "            \n",
    "            predictions = model(cat, text, mask)\n",
    "            \n",
    "            softmax_predictions = F.softmax(predictions, dim=1)\n",
    "            binary_predictions = 1 - softmax_predictions[:, 0]\n",
    "            \n",
    "            preds += binary_predictions.tolist()\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9129edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-3d3759029939>:12: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  labels = T.tensor(labels, dtype=T.long)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5c3cf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [1 - p for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f8fc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('DMC-phase2-validation.parquet', engine='fastparquet')\n",
    "pred_df = pd.DataFrame()\n",
    "pred_df['post_id'] = test_df['post_id']\n",
    "pred_df['predictions'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f4c925f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c16685db-c7b2-403e-b56d-4a745d7e4686</td>\n",
       "      <td>0.981617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e65f2de9-acd2-4f03-9395-24f89e1fed32</td>\n",
       "      <td>0.228582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdf973fe-0b45-49d5-b5d6-bbca65c87adc</td>\n",
       "      <td>0.004238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e29d3726-6f7e-42f2-9684-26f1cd3405f8</td>\n",
       "      <td>0.988345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37fb59d9-be82-4985-84ed-9132732b2144</td>\n",
       "      <td>0.001131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                post_id  predictions\n",
       "0  c16685db-c7b2-403e-b56d-4a745d7e4686     0.981617\n",
       "1  e65f2de9-acd2-4f03-9395-24f89e1fed32     0.228582\n",
       "2  cdf973fe-0b45-49d5-b5d6-bbca65c87adc     0.004238\n",
       "3  e29d3726-6f7e-42f2-9684-26f1cd3405f8     0.988345\n",
       "4  37fb59d9-be82-4985-84ed-9132732b2144     0.001131"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72fdb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.4",
   "language": "python",
   "name": "3.9.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
