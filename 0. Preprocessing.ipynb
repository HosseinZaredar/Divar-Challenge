{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f0928c",
   "metadata": {},
   "source": [
    "### Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afe1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('DMC-Train.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b97871",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('DMC-phase2-validation.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc3d5b",
   "metadata": {},
   "source": [
    "### Unwrap JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b29f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4731b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_json(dataframe):\n",
    "    post_df = pd.json_normalize(dataframe.post_data.apply(json.loads))\n",
    "    post_df = post_df.drop(post_df.columns[15:], axis=1)\n",
    "    dataframe = pd.concat([post_df, dataframe], axis=1)\n",
    "    dataframe = dataframe.drop('post_data', axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2641b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = unwrap_json(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9deca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = unwrap_json(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafd74a",
   "metadata": {},
   "source": [
    "### Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: binary\n",
    "\n",
    "train_df['review_label'] = train_df['review_label'].map({'accept': 1, 'reject': 0})\n",
    "train_label = train_df[['review_label']].to_numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee0bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: multi-label\n",
    "\n",
    "train_df['reject_reason_id'] = train_df['reject_reason_id'].map({\n",
    "    0: 0, 5: 1, 12: 2, 13: 3, 29: 4, 139: 5, 145: 6, 146: 7, 163: 8})\n",
    "train_label = train_df[['reject_reason_id']].to_numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f962251",
   "metadata": {},
   "source": [
    "### Categorical to One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c427541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03811ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_onehot(dataframe):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(dataframe)\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45925cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['body_status', 'brand_model', 'color', 'document', 'gearbox',\n",
    "               'selling_type', 'year', 'third_party_insurance_deadline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c3bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_model = train_onehot(train_df[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = onehot_model.transform(train_df[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b747e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = onehot_model.transform(test_df[cat_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadad7e",
   "metadata": {},
   "source": [
    "### Numerical to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc949bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_num2cat(dataframe, num_bins):\n",
    "    bins = pd.qcut(dataframe, duplicates='drop', q=num_bins, retbins=True)[1]\n",
    "    bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "    res_df = pd.cut(dataframe, bins).to_frame()\n",
    "    oh_model = train_onehot(res_df)\n",
    "    return bins, oh_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6317006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_num2cat(dataframe, model, bins):\n",
    "    res_df = pd.cut(dataframe, bins).to_frame()\n",
    "    return model.transform(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2af344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_bins, price_model = train_num2cat(train_df['new_price'], 60)\n",
    "usage_bins, usage_model = train_num2cat(train_df['usage'], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b6f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_price = transform_num2cat(train_df['new_price'], price_model, price_bins)\n",
    "train_usage = transform_num2cat(train_df['usage'], usage_model, usage_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c2a19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = hstack((train_cat, train_price, train_usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c93fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_price = transform_num2cat(test_df['new_price'], price_model, price_bins)\n",
    "test_usage = transform_num2cat(test_df['usage'], usage_model, usage_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "136971c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = hstack((test_cat, test_price, test_usage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15208a",
   "metadata": {},
   "source": [
    "### Cleaning Text Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "663dd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "from hazm import Normalizer, WordTokenizer\n",
    "import re\n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a320c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translation_dict.pickle', 'rb') as handle:\n",
    "    new_translation = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "490ba89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHandler:\n",
    "    def __init__(self, persian_numbers=False,\n",
    "                 change_lang_spacing=True,\n",
    "                 remove_non_standard_char=True,\n",
    "                 remove_repetitive_chars=True,\n",
    "                 user_translations=None,\n",
    "                 stopwords=None,\n",
    "                 lemma=False\n",
    "                ):\n",
    "        \n",
    "        if not persian_numbers:\n",
    "            number_src = '۰۱۲۳۴۵۶۷۸۹٪'\n",
    "            number_dest = '0123456789%'\n",
    "        else:\n",
    "            number_dest = '۰۱۲۳۴۵۶۷۸۹٪'\n",
    "            number_src = '0123456789%'\n",
    "        \n",
    "        self.number_translations = self.maketrans(number_src, number_dest)\n",
    "        \n",
    "        if not user_translations:\n",
    "            self.user_translations = dict()\n",
    "        else:\n",
    "            self.user_translations = user_translations\n",
    "\n",
    "        self._remove_repetitive_chars = remove_repetitive_chars\n",
    "        self._change_lang_spacing = change_lang_spacing\n",
    "        self._remove_non_standard_char = remove_non_standard_char\n",
    "        self._stopwords = stopwords\n",
    "        \n",
    "        self.text_normalizer = hazm.Normalizer(\n",
    "            remove_extra_spaces=True,\n",
    "            persian_style=False,\n",
    "            persian_numbers=False,\n",
    "            remove_diacritics=True,\n",
    "            affix_spacing=True,\n",
    "            token_based=False,\n",
    "            punctuation_spacing=True)\n",
    "\n",
    "        self.word_tokenizer = hazm.WordTokenizer(\n",
    "            join_verb_parts=False,\n",
    "            separate_emoji=True,\n",
    "            replace_links=True,\n",
    "            replace_IDs=False,\n",
    "            replace_emails=True,\n",
    "            replace_numbers=False,\n",
    "            replace_hashtags=False)\n",
    "\n",
    "    def normalize(self, text: str):\n",
    "        text = text.translate(self.user_translations)\n",
    "        text = text.translate(self.number_translations)\n",
    "        \n",
    "        text = text.lower()\n",
    "\n",
    "        normalized_text = self.text_normalizer.normalize(text)\n",
    "\n",
    "        if self._remove_repetitive_chars:\n",
    "            text = self.remove_rep_chars(text)\n",
    "\n",
    "        if self._change_lang_spacing:\n",
    "            text = self.change_lang_spacing(text)\n",
    "\n",
    "        if self._remove_non_standard_char:\n",
    "            text = self.remove_non_standard_char(text)\n",
    "        \n",
    "        text = re.sub(r'[\\u200c\\s]*\\s[\\s\\u200c]*', ' ', text)\n",
    "        text = re.sub(r'[\\u200c]+', '\\u200c', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def maketrans(src_chars, dest_chars):\n",
    "        return dict((ord(a), b) for a, b in zip(src_chars, dest_chars))\n",
    "    \n",
    "    @staticmethod\n",
    "    def change_lang_spacing(text: str) -> str:\n",
    "        return re.sub('(([a-zA-Z0-9/\\-\\.]+)|([ء-یژپچگ]+))', r' \\1 ', text).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_non_standard_char(text: str) -> str:\n",
    "        return re.sub(r'[^a-zA-Z0-9\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF]', ' ', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_rep_chars(text: str) -> str:\n",
    "        return re.sub(r'([^0-9])\\1\\1+', r'\\1', text)\n",
    "    \n",
    "    def preprocess_text(self, text: str):\n",
    "        normalized_text = self.normalize(text)\n",
    "        return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c8f0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_handler = TextHandler(user_translations=new_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bca34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['prep_title'] = train_df['title'].apply(text_handler.preprocess_text)\n",
    "train_df['prep_description'] = train_df['description'].apply(text_handler.preprocess_text)\n",
    "train_df['prep_text'] = '<start> ' + train_df['prep_description'] + ' <delim> ' + train_df['prep_title'] + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3715be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prep_title'] = test_df['title'].apply(text_handler.preprocess_text)\n",
    "test_df['prep_description'] = test_df['description'].apply(text_handler.preprocess_text)\n",
    "test_df['prep_text'] = '<start> ' + test_df['prep_description'] + ' <delim> ' + test_df['prep_title'] + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4aeae4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['prep_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ebe1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df['prep_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00c50d",
   "metadata": {},
   "source": [
    "### Saving to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfca4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b142d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(train_texts, f)\n",
    "    \n",
    "save_npz(\"train_cat.npz\", train_cat)\n",
    "\n",
    "with open('train_label.npy', 'wb') as f:\n",
    "    np.save(f, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a847f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(test_texts, f)\n",
    "    \n",
    "save_npz(\"test_cat.npz\", test_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.4",
   "language": "python",
   "name": "3.9.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
