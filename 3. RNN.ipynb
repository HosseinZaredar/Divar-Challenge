{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357092d7",
   "metadata": {},
   "source": [
    "### Loading from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226614b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7cbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_texts.pkl', 'rb') as f:\n",
    "    train_texts = pickle.load(f)\n",
    "    \n",
    "train_cat = load_npz(\"train_cat.npz\")\n",
    "\n",
    "with open('train_label.npy', 'rb') as f:\n",
    "    train_label = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a21cd",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34406050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91605aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train_cat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94bce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60957c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train_texts)\n",
    "indices = np.arange(train_len)\n",
    "np.random.shuffle(indices, )\n",
    "train_idx = indices[:math.floor(0.9*train_len)]\n",
    "val_idx = indices[math.floor(0.9*train_len):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f15012",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = [train_texts[i] for i in val_idx]\n",
    "val_cat = train_cat[val_idx]\n",
    "val_label = train_label[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca77c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [train_texts[i] for i in train_idx]\n",
    "train_cat = train_cat[train_idx]\n",
    "train_label = train_label[train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebcd78",
   "metadata": {},
   "source": [
    "### Creating PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f6d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81c1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_df):\n",
    "        self.itos = {0: '<pad>', 1: '<unk>'}\n",
    "        self.stoi = {'<pad>': 0, '<unk>': 1}\n",
    "        self.min_df = min_df\n",
    "        self.tokenizer = hazm.WordTokenizer(\n",
    "            join_verb_parts=False,\n",
    "            separate_emoji=True,\n",
    "            replace_links=True,\n",
    "            replace_IDs=False,\n",
    "            replace_emails=True,\n",
    "            replace_numbers=False,\n",
    "            replace_hashtags=False\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 2\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer.tokenize(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.min_df:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        tokenized_text = tokenized_text[:128]\n",
    "        \n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi['<unk>']\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae72db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivarDataset(Dataset):\n",
    "    def __init__(self, cat_mat, text_list, labels, vocab):\n",
    "        self.cat_mat = cat_mat\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "            \n",
    "        self.text_list = [self.vocab.numericalize(text) for text in text_list]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        cat_row = self.cat_mat[index]\n",
    "        numer_text = self.text_list[index]\n",
    "        label = self.labels[index]\n",
    "        return T.tensor(cat_row, dtype=T.float32), T.tensor(numer_text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a562ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        cat_rows = [item[0] for item in batch]\n",
    "        cat_rows = T.vstack(cat_rows)\n",
    "        numer_texts = [item[1] for item in batch]\n",
    "        text_lengths = [text.shape[0] for text in numer_texts]\n",
    "        numer_texts = pad_sequence(numer_texts, batch_first=True, padding_value=self.pad_idx)\n",
    "        labels = [item[2] for item in batch]\n",
    "        labels = T.tensor(labels, dtype=T.long)\n",
    "        \n",
    "        return cat_rows, numer_texts, labels, text_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012df394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(cat_mat, text_list, labels, vocab, batch_size=32, shuffle=True):\n",
    "    \n",
    "    dataset = DivarDataset(cat_mat, text_list, labels, vocab)\n",
    "    \n",
    "    pad_idx = dataset.vocab.stoi['<pad>']\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=Collate(pad_idx=pad_idx)\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f287444",
   "metadata": {},
   "source": [
    "### Creating Loader Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e28f3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5f8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=25\n",
    "vocab = Vocabulary(min_df)\n",
    "vocab.build_vocabulary(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f36d1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(train_cat, train_texts, train_label, vocab, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75673737",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = get_loader(val_cat, val_texts, val_label, vocab, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0672993",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d4c0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9224894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, cat_dim, dict_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(dict_dim, embedding_dim, padding_idx=0, dtype=T.float32)\n",
    "        self.lstm = nn.GRU(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.cat_shrink = nn.Sequential(\n",
    "            nn.Linear(cat_dim, 600),\n",
    "            nn.Tanh(),            \n",
    "        )\n",
    "                \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1200, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 9)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, cat, text, text_lengths):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.lstm(packed_embedded)\n",
    "                \n",
    "        hidden = T.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "\n",
    "        cat_shrinked = self.cat_shrink(cat)\n",
    "        \n",
    "        lin_input = T.cat((cat_shrinked, hidden), dim=1)\n",
    "        \n",
    "        return self.fc(lin_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc636b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_DIM = train_cat.shape[1]\n",
    "DICT_DIM = train_loader.dataset.vocab.__len__()\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "model = Classifier(CAT_DIM, DICT_DIM, EMBEDDING_DIM, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969d4cb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e5bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebd0a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), weight_decay=0.2)\n",
    "criterion = nn.CrossEntropyLoss(weight=T.tensor([1.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b76b0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "criterion = criterion.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37243e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = T.round(preds)\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4790142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    for i, (cat, text, label, text_lengths) in enumerate(iterator):\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f'    mini-batch {i}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cat = cat.to(device)\n",
    "        text = text.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        predictions = model(cat, text, text_lengths)\n",
    "        loss = criterion(predictions, label)\n",
    "        \n",
    "        softmax_predictions = F.softmax(predictions, dim=1)\n",
    "        binary_predictions = 1 - softmax_predictions[:, 0]\n",
    "        binary_label = (label != 0).float()\n",
    "        acc = binary_accuracy(binary_predictions, binary_label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(text)\n",
    "        epoch_acc += acc.item() * len(text)\n",
    "                \n",
    "    return epoch_loss / len(iterator.dataset), epoch_acc / len(iterator.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abfe4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    with T.no_grad():\n",
    "    \n",
    "        for i, (cat, text, label, text_lengths) in enumerate(iterator):\n",
    "\n",
    "            cat = cat.to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            predictions = model(cat, text, text_lengths).squeeze(1)\n",
    "                            \n",
    "            loss = criterion(predictions, label)\n",
    "            \n",
    "            softmax_predictions = F.softmax(predictions, dim=1)\n",
    "            binary_predictions = 1 - softmax_predictions[:, 0]\n",
    "            binary_label = (label != 0).float()\n",
    "            acc = binary_accuracy(binary_predictions, binary_label)\n",
    "            \n",
    "            preds += binary_predictions.tolist()\n",
    "            labels += binary_label.tolist()\n",
    "\n",
    "            epoch_loss += loss.item() * len(text)\n",
    "            epoch_acc += acc.item() * len(text)\n",
    "            \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "        \n",
    "    return epoch_loss / len(iterator.dataset), epoch_acc / len(iterator.dataset), auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8bc0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_early_stop(model, train_iterator, val_iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    best_auc = 0\n",
    "        \n",
    "    for i, (cat, text, label, text_lengths) in enumerate(train_iterator):\n",
    "        \n",
    "        if i % 250 == 0:\n",
    "            _, _, valid_auc = evaluate(model, val_iterator, criterion)\n",
    "            \n",
    "            T.save(model.state_dict(), f'fine_gru_{valid_auc}.torch')\n",
    "            print(f'    mini-batch {i} val auc= {valid_auc}')\n",
    "            \n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cat = cat.to(device)\n",
    "        text = text.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        predictions = model(cat, text, text_lengths)\n",
    "        loss = criterion(predictions, label)\n",
    "        \n",
    "        softmax_predictions = F.softmax(predictions, dim=1)\n",
    "        binary_predictions = 1 - softmax_predictions[:, 0]\n",
    "        binary_label = (label != 0).float()\n",
    "        acc = binary_accuracy(binary_predictions, binary_label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(text)\n",
    "        epoch_acc += acc.item() * len(text)\n",
    "                  \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87332674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8a8d55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "    mini-batch 1000\n",
      "    mini-batch 1500\n",
      "Epoch Time: 6m 43s\n",
      "    Train Loss: 0.547 | Train Acc: 89.25%\n",
      "     Val. Loss: 0.443 |  Val. Acc: 90.96% | Val. AUC: 0.9583\n",
      "\n",
      "Epoch: 2\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "    mini-batch 1000\n",
      "    mini-batch 1500\n",
      "Epoch Time: 6m 43s\n",
      "    Train Loss: 0.388 | Train Acc: 92.64%\n",
      "     Val. Loss: 0.382 |  Val. Acc: 93.42% | Val. AUC: 0.9685\n",
      "\n",
      "Epoch: 3\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "    mini-batch 1000\n",
      "    mini-batch 1500\n",
      "Epoch Time: 6m 48s\n",
      "    Train Loss: 0.334 | Train Acc: 93.50%\n",
      "     Val. Loss: 0.365 |  Val. Acc: 92.13% | Val. AUC: 0.9718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_auc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "       \n",
    "    print(f'Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'    Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'     Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. AUC: {valid_auc:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3312bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8f52eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "    mini-batch 0\n",
      "    mini-batch 500\n",
      "    mini-batch 1000\n",
      "    mini-batch 1500\n",
      "Epoch Time: 6m 42s\n",
      "    Train Loss: 0.228 | Train Acc: 95.31%\n",
      "     Val. Loss: 0.330 |  Val. Acc: 94.06% | Val. AUC: 0.9767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_auc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "       \n",
    "    print(f'Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'    Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'     Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. AUC: {valid_auc:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628586d",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8986f088",
   "metadata": {},
   "source": [
    "0: 0, 5: 1, 12: 2, 13: 3, 29: 4, 139: 5, 145: 6, 146: 7, 163: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3eb716a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96     41471\n",
      "           1       0.82      0.86      0.84       170\n",
      "           2       0.86      0.90      0.88      2740\n",
      "           3       0.80      0.83      0.82      2143\n",
      "           4       0.74      0.61      0.67       137\n",
      "           5       0.67      0.71      0.69      2158\n",
      "           6       0.97      0.96      0.97      2702\n",
      "           7       0.78      0.83      0.80       267\n",
      "           8       0.73      0.80      0.76      2249\n",
      "\n",
      "    accuracy                           0.93     54037\n",
      "   macro avg       0.81      0.83      0.82     54037\n",
      "weighted avg       0.93      0.93      0.93     54037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7838e",
   "metadata": {},
   "source": [
    "### Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a8d9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.save(model.state_dict(), 'gru_9767.torch')\n",
    "T.save(optimizer.state_dict(), 'optim_gru_9763.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18e11343",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_stoi_gru_9767.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab.stoi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e8a8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_itos_gru_9767.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab.itos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cc58e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340 10652\n"
     ]
    }
   ],
   "source": [
    "print(CAT_DIM, DICT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74507902",
   "metadata": {},
   "source": [
    "### Loading Test Data & The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5084c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "import hazm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2647126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_texts.pkl', 'rb') as f:\n",
    "    test_texts = pickle.load(f)\n",
    "    \n",
    "test_cat = load_npz(\"test_cat.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afad0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = test_cat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7517ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_stoi_gru_9767.pkl', 'rb') as f:\n",
    "    vocab_stoi = pickle.load(f)\n",
    "\n",
    "with open('vocab_itos_gru_9767.pkl', 'rb') as f:\n",
    "    vocab_itos = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc5645fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(25)\n",
    "vocab.stoi = vocab_stoi\n",
    "vocab.itos = vocab_itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bb23aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_loader(test_cat, test_texts, np.zeros(len(test_texts)), vocab, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad67c649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAT_DIM = 1340 \n",
    "DICT_DIM = 10652\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "model = Classifier(CAT_DIM, DICT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model.load_state_dict(T.load('gru_9767.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc25172",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(T.load('optim_gru_9767.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58fc69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2396e",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7bfbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65ea3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    with T.no_grad():\n",
    "        for i, (cat, text, label, text_lengths) in enumerate(iterator):\n",
    "\n",
    "            cat = cat.to(device)\n",
    "            text = text.to(device)\n",
    "            \n",
    "            predictions = model(cat, text, text_lengths)\n",
    "            \n",
    "            softmax_predictions = F.softmax(predictions, dim=1)\n",
    "            binary_predictions = 1 - softmax_predictions[:, 0]\n",
    "            \n",
    "            preds += binary_predictions.tolist()\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9129edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-3d3759029939>:12: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  labels = T.tensor(labels, dtype=T.long)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c3cf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [1 - p for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f8fc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('DMC-phase2-validation.parquet', engine='fastparquet')\n",
    "pred_df = pd.DataFrame()\n",
    "pred_df['post_id'] = test_df['post_id']\n",
    "pred_df['predictions'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46640e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69a9fbfc-b710-4c5f-8f2f-67a204b235ea</td>\n",
       "      <td>0.987511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cb7b6f33-de2b-4e73-bc78-686c9e04ca00</td>\n",
       "      <td>0.970797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>396bccd3-29da-489c-b607-afeec8ab53bd</td>\n",
       "      <td>0.979923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4e11ee23-d3fe-4c07-a66d-25c96e507cb2</td>\n",
       "      <td>0.996737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d78d9ccb-249b-465b-9f5a-3c88851ffc10</td>\n",
       "      <td>0.984913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                post_id  predictions\n",
       "0  69a9fbfc-b710-4c5f-8f2f-67a204b235ea     0.987511\n",
       "1  cb7b6f33-de2b-4e73-bc78-686c9e04ca00     0.970797\n",
       "2  396bccd3-29da-489c-b607-afeec8ab53bd     0.979923\n",
       "3  4e11ee23-d3fe-4c07-a66d-25c96e507cb2     0.996737\n",
       "4  d78d9ccb-249b-465b-9f5a-3c88851ffc10     0.984913"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72fdb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.4",
   "language": "python",
   "name": "3.9.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
